{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation du client Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('KMeans-project').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture du fichier properties.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../propriete/properties.conf\")\n",
    "pathin = config['Bristol-City-bike']['Input-data']\n",
    "pathout = config['Bristol-City-bike']['Output-data']\n",
    "partkmeans = config['Bristol-City-bike']['Kmeanslevel']\n",
    "partkmeans = int(partkmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import du fichier Bristol-city-bike.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bristol = spark.read.json(pathin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du DataFrame kmeansdf contenant seulement latitude et longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansdf = bristol.select(\"latitude\", \"longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "features = ('latitude', 'longitude')\n",
    "kmeans = KMeans().setK(partkmeans).setSeed(1)\n",
    "assembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n",
    "dataset = assembler.transform(kmeansdf)\n",
    "model = kmeans.fit(dataset)\n",
    "fitted = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colonnes de fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude', 'longitude', 'features', 'prediction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude moyenne et Longitude moyenne pour chaque groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.createOrReplaceTempView(\"fittedSQL\")\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En SQL\n",
    "spark.sql(\"\"\"select prediction, Mean(latitude) as MoyLatitude, Mean(longitude) as MoyLongitude \n",
    "            from fittedSQL group by prediction order by prediction\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En DSL\n",
    "fitted.groupby('prediction').agg(F.mean('latitude').alias('MoyLatitude'), F.mean('longitude').alias('MoyLongitude')).orderBy('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : Visualisation dans une map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "bristol_coords = [-27.4710107, 153.0234489]\n",
    "#Create the map\n",
    "my_map = folium.Map(location = bristol_coords, zoom_start = 14)\n",
    "#Add markers to the map\n",
    "for i in range(fitted.count()):\n",
    "    if(fitted.collect()[i][3]==0):\n",
    "        clust0 = [fitted.collect()[i][0], fitted.collect()[i][1]]\n",
    "        folium.Marker(clust0, popup = 'Cluster 0', icon=folium.Icon(color='blue')).add_to(my_map)\n",
    "    elif(fitted.collect()[i][3]==1):\n",
    "        clust1 = [fitted.collect()[i][0], fitted.collect()[i][1]]\n",
    "        folium.Marker(clust1, popup = 'Cluster 1', icon=folium.Icon(color='red')).add_to(my_map)\n",
    "    else :\n",
    "        clust2 = [fitted.collect()[i][0], fitted.collect()[i][1]]\n",
    "        folium.Marker(clust2, popup = 'Cluster 2', icon=folium.Icon(color='green')).add_to(my_map)\n",
    "#Display the map\n",
    "my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export du DataFrame fitted après élimination de la colonne features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.drop(\"features\").write.csv(pathout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
